{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> \n",
    "Pipeline\n",
    "</h1> \n",
    "\n",
    "Please cite the following paper when using the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'Setting.py'` not found.\n",
      "ERROR:root:File `'PIA.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import Setting\n",
    "import PIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_result_from_data(data_dir, result_dir, dp_dir):\n",
    "    \"\"\"\n",
    "    Get result from data\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : the pathname of the data directory\n",
    "    result_dir : the pathname of the result directory\n",
    "    dp_dir : the pathname of the DataPreprocessing module directory\n",
    "    \"\"\"\n",
    "\n",
    "    # Add code_dir folder\n",
    "    sys.path.append(dp_dir)\n",
    "    \n",
    "    # Import the DataPreprocessing module\n",
    "    %run DataPreprocessing\n",
    "    # Get the DataPreprocessing object\n",
    "    dp = DataPreprocessing.DataPreprocessing(data_dir)\n",
    "\n",
    "    # Match data file with names file\n",
    "    data_names = dp.match_data_names()\n",
    "\n",
    "    # The parallel pipelines for data preprocessing, train, test, and evaluate the ALA classifier\n",
    "    # n_jobs = -1 indicates (all CPUs are used)\n",
    "    # Set backend=\"multiprocessing\" (default) to prevent sharing memory between parent and threads\n",
    "    Parallel(n_jobs=-1)(delayed(pipeline)(dp, data_file, names_file, result_dir)\n",
    "                        for data_file, names_file in data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline(dp, data_files, names_file, result_dir):\n",
    "    \"\"\"\n",
    "    The pipeline for data preprocessing, principle interaction analysis (PIA), train, test, and evaluate the classifiers\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dp : the DataPreprocessing module\n",
    "    data_files : the pathname of the data files\n",
    "    names_file : the pathname of the names file\n",
    "    result_dir : the pathname of the result directory\n",
    "    \"\"\"\n",
    "\n",
    "    # Data preprocessing: get the Setting, Names, and Data object\n",
    "    setting, names, data = dp.get_setting_names_data(data_files, names_file, result_dir, Setting)\n",
    "    \n",
    "    # Get the PIA object\n",
    "    pia = PIA(setting.min_samples_importance, setting.min_samples_interaction, setting.p_val, setting.random_state)\n",
    "    # The fit-transform\n",
    "    data.X_train_I = pia.fit_transform(data.X_train, data.y_train)\n",
    "    # The transform\n",
    "    data.X_test_I = pia.transform(X_test)\n",
    "    # Update names.features_I\n",
    "    for class_ in pia.keys():\n",
    "        for I, prob in pia.D[class_]:\n",
    "            names.features_I.append([names.features[c] for c in I])\n",
    "\n",
    "    # Write the interaction file\n",
    "    write_interaction_file(setting, names, pia)\n",
    "        \n",
    "    # Train, test, and evaluate the classifier\n",
    "    # Set backend=\"multiprocessing\" (default) to prevent sharing memory between parent and threads\n",
    "    Parallel(n_jobs=setting.n_jobs)(delayed(train_test_eval)(setting, names, data, clf_name, pia)\n",
    "                                    for clf_name in setting.classifiers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_interaction_file(setting, names, pia):\n",
    "    \"\"\"\n",
    "    Write the interaction file\n",
    "    \n",
    "    setting: the Setting object\n",
    "    names : the Names object\n",
    "    pia : the PIA object\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the pathname of the interaction file\n",
    "    interaction_file = setting.interaction_file_dir + setting.interaction_file_name + setting.interaction_file_type\n",
    "\n",
    "    # Make directory\n",
    "    directory = os.path.dirname(interaction_file)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    with open(interaction_file, 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"class, interaction, probability\" + '\\n')\n",
    "\n",
    "        # For each class of the target\n",
    "        for class_ in sorted(pia.D.keys()):\n",
    "            for I, prob in pia.D[class_]:\n",
    "                f.write(class_ + ', '.join([names.features[c] for c in I]) + ', ' + str(prob) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_eval(setting, names, data, clf_name, pia):\n",
    "    \"\"\"\n",
    "    Train, test, and evaluate the classifier\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    setting : the Setting object\n",
    "    names : the Names object\n",
    "    data : the Data object\n",
    "    clf_name : the name of the classifier\n",
    "    pia : the PIA object\n",
    "    \"\"\"\n",
    "\n",
    "    classifier = setting.classifiers[clf_name]\n",
    "\n",
    "    if clf_name == 'RandomForestClassifier':\n",
    "        clf = classifier(random_state=setting.random_state, n_jobs=setting.n_jobs)\n",
    "        clf_I = classifier(random_state=setting.random_state, n_jobs=setting.n_jobs)\n",
    "    elif clf_name == 'AdaBoostClassifier':\n",
    "        clf = classifier(random_state=setting.random_state)\n",
    "        clf_I = classifier(random_state=setting.random_state)\n",
    "    elif clf_name == 'MLPClassifier':\n",
    "        clf = classifier(random_state=setting.random_state)\n",
    "        clf_I = classifier(random_state=setting.random_state)\n",
    "    elif clf_name == 'KNeighborsClassifier':\n",
    "        clf = classifier(n_jobs=setting.n_jobs)\n",
    "        clf_I = classifier(n_jobs=setting.n_jobs)\n",
    "    elif clf_name == 'GaussianNB':\n",
    "        clf = classifier()\n",
    "        clf_I = classifier()\n",
    "    elif clf_name == 'DecisionTreeClassifier':\n",
    "        clf = classifier(random_state=setting.random_state)\n",
    "        clf_I = classifier(random_state=setting.random_state)\n",
    "    elif clf_name == 'LogisticRegression':\n",
    "        clf = classifier(random_state=setting.random_state, n_jobs=setting.n_jobs)\n",
    "        clf_I = classifier(random_state=setting.random_state, n_jobs=setting.n_jobs)\n",
    "    elif clf_name == 'GaussianProcessClassifier':\n",
    "        clf = classifier(random_state=setting.random_state, n_jobs=setting.n_jobs)\n",
    "        clf_I = classifier(random_state=setting.random_state, n_jobs=setting.n_jobs)\n",
    "    elif clf_name == 'SVC':\n",
    "        clf = classifier(random_state=setting.random_state)\n",
    "        clf_I = classifier(random_state=setting.random_state)\n",
    "\n",
    "    # Train clf\n",
    "    clf.fit(data.X_train, data.y_train)\n",
    "\n",
    "    # Test clf\n",
    "    y_pred = clf.predict(data.X_test)\n",
    "    \n",
    "    # Train clf_I, with interaction\n",
    "    clf_I.fit(data.X_train_I, data.y_train)\n",
    "\n",
    "    # Test clf, with interaction\n",
    "    y_pred_I = clf_I.predict(data.X_test_I)\n",
    "\n",
    "    # Evaluate clf\n",
    "    eval(setting, names, data, clf, clf_I, y_pred, y_pred_I, clf_name, pia)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(setting, names, data, clf, clf_I, y_pred, y_pred_I, clf_name, pia):\n",
    "    \"\"\"\n",
    "    Evaluate the classifier\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    setting: the Setting object\n",
    "    names: the Names object\n",
    "    data: the Data object\n",
    "    clf: the classifier\n",
    "    clf_I: the classifier, with interaction\n",
    "    y_pred: the predicted values of the target\n",
    "    y_pred_I: the predicted values of the target, with interaction\n",
    "    clf_name: the name of the classifier\n",
    "    pia : the PIA object\n",
    "    \"\"\"\n",
    "\n",
    "    setting.set_plt()\n",
    "\n",
    "    if setting.score_file_dir is not None:\n",
    "        # Write the score file\n",
    "        write_score_file(setting, data.y_test, y_pred, y_pred_I, clf_name)\n",
    "\n",
    "    if (setting.prob_dist_fig_dir is not None\n",
    "        and (isinstance(clf, setting.classifiers['RandomForestClassifier']) is True)):\n",
    "        # Plot the feature importance figures\n",
    "        plot_feature_importance_fig(setting, names, clf, clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_score_file(setting, y_test, y_pred, y_pred_I, clf_name):\n",
    "    \"\"\"\n",
    "    Write the score file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    setting: the Setting object\n",
    "    y_test: the actual values of the target\n",
    "    y_pred: the predicted values of the target\n",
    "    y_pred_I: the predicted values of the target, with interaction\n",
    "    clf_name: the name of the classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the directory of the score file\n",
    "    score_file_dir = setting.score_file_dir + clf_name + '/'\n",
    "    # Get the pathname of the score file\n",
    "    score_file = score_file_dir + setting.score_file_name + setting.score_file_type\n",
    "\n",
    "    # Make directory\n",
    "    directory = os.path.dirname(score_file)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    with open(score_file, 'w') as f:\n",
    "        for average in setting.average:      \n",
    "            precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average=average)\n",
    "            # Write the precision, recall, and fscore\n",
    "            f.write(\"precision, recall, fscore using \" + average + ':' + '\\n')\n",
    "            f.write(str(precision) + ', ' + str(recall) + ', ' + str(fscore) + '\\n\\n')\n",
    "         \n",
    "            precision_I, recall_I, fscore_I, support_I = precision_recall_fscore_support(y_test, y_pred_I, average=average)\n",
    "            # Write the precision, recall, and fscore, with interaction\n",
    "            f.write(\"precision, recall, fscore using \" + average + ', with interaction:' + '\\n')\n",
    "            f.write(str(precision_I) + ', ' + str(recall_I) + ', ' + str(fscore_I) + '\\n\\n')\n",
    "            \n",
    "            dif_precision, dif_recall, dif_fscore = precision_I - precison, recall_I - recall, fscore_I - fscore\n",
    "            # Write dif_precision, dif_recall, and dif_fscore\n",
    "            f.write(\"precision (with interaction) - precison, recall (with interaction) - recall, fscore (with interaction) - fscore:\" + '\\n')\n",
    "            f.write(str(dif_precision) + ', ' + str(dif_recall) + ', ' + str(dif_fscore) + '\\n\\n')\n",
    "            \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        # Write the accuracy\n",
    "        f.write(\"accuracy:\" + '\\n')\n",
    "        f.write(str(accuracy) + '\\n')\n",
    "        \n",
    "        accuracy_I = accuracy_score(y_test, y_pred_I)\n",
    "        # Write the accuracy, with interaction\n",
    "        f.write(\"accuracy, with interaction:\" + '\\n')\n",
    "        f.write(str(accuracy_I) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance_fig(setting, names, clf, clf_name):\n",
    "    \"\"\"\n",
    "    Plot the feature importance figures\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    setting : the Setting object\n",
    "    names : the Names object\n",
    "    clf : the classifier\n",
    "    clf_name: the name of the classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the directory of the feature importance file\n",
    "    feature_importance_fig_dir = setting.feature_importance_fig_dir + clf_name + '/'\n",
    "    # Get the pathname of the feature importance figure\n",
    "    feature_importance_fig = (feature_importance_fig_dir + setting.feature_importance_fig_name + setting.feature_importance_fig_type)\n",
    "\n",
    "    # Make directory\n",
    "    directory = os.path.dirname(feature_importance_fig)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Get the feature importances\n",
    "    importances = clf.feature_importances_\n",
    "    \n",
    "    # Convert the importances into one-dimensional 1darray with corresponding df column names as axis labels\n",
    "    f_importances = pd.Series(importances, names.features_I)\n",
    "\n",
    "    # Sort the array in descending order of the importances\n",
    "    f_importances.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    # Make the bar plot from f_importances \n",
    "    f_importances.plot(kind='bar', figsize=(16,9), rot=90, fontsize=30)\n",
    "\n",
    "    plt.xlabel('Feature', fontsize=30)\n",
    "    plt.ylabel('Importance', fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(feature_importance_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9116c49149a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Get the pathname of the DataPreprocessing module directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Get result from data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Get the pathname of the data directory from command line\n",
    "    data_dir = sys.argv[1]\n",
    "\n",
    "    # Get the pathname of the result directory from command line\n",
    "    result_dir = sys.argv[2]\n",
    "\n",
    "    # Get the pathname of the DataPreprocessing module directory\n",
    "    dp_dir = sys.argv[3]\n",
    "\n",
    "    # Get result from data\n",
    "    get_result_from_data(data_dir, result_dir, dp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
